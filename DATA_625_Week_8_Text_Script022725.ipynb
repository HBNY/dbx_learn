{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HBNY/dbx_learn/blob/main/DATA_625_Week_8_Text_Script022725.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz8GmEYHW0_j",
        "outputId": "95025220-7e22-40a3-f38d-91509e996807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# DATA 625 Week 8 Word Counting Script\n",
        "# You only need to run this top cell once per run session\n",
        "\n",
        "#This code is preparing a Python environment for text analysis.\n",
        "\n",
        "#  importing various libraries\n",
        "import sys  # Used for system-related operations and input/output.\n",
        "import string  # Used for working with strings and character manipulation.\n",
        "import matplotlib.pyplot as plt  # Used for creating plots and data visualizations.\n",
        "import pandas as pd  # Used for data manipulation and analysis using DataFrames.\n",
        "import numpy as np  # Used for numerical operations and array manipulation.\n",
        "\n",
        "# importing libraries and modues that are essential for text analysis\n",
        "import nltk #This is a bit redundant, but necessary for the Try Jupyter environment.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# downloading some necessary data for the NLTK library, including stopwords,\n",
        "# punctuation, and the WordNet lexical database. These resources are used later for text preprocessing.\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Run this cell each time you want to analyze a new file\n",
        "\n",
        "#Here we load in the file based on the filename.\n",
        "#If the filename doesn't exist, the program exits.\n",
        "\n",
        "#user enters file name\n",
        "filename = input('Enter file name : ')\n",
        "\n",
        "# adding the folder location so colab knows where to find the file\n",
        "fullname =  '/content/' + filename\n",
        "\n",
        "try:\n",
        "    with open(fullname, 'r') as f:\n",
        "      text = f.read()\n",
        "      print(\"File read successfully!\")\n",
        "      print(\"Here are the first 100 characters of your file:\")\n",
        "      print(text[0:100])\n",
        "except:\n",
        "    print('File cannot be opened: ', filename)\n",
        "\n",
        "#Here we load in the stopwords from the downloaded nltk stopwords list.\n",
        "#\n",
        "stop = stopwords.words('english')\n",
        "# Now we append a few words additional words that do not provide much meaning\n",
        "stop.append(\"year\")\n",
        "stop.append(\"let\")\n",
        "stop.append(\"year\")\n",
        "stop.append(\"you\")\n",
        "stop.append(\"your\")\n",
        "stop.append(\"last\")\n",
        "stop.append(\"us\")\n",
        "\n",
        "# The purpose of this chunk is to prepare our text and then get the most frequently occurring words\n",
        "\n",
        "#Set the number of words to return\n",
        "num_words = 30\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "# Word stemming simplifies words to their base form, helping with text analysis by grouping similar words together.\n",
        "lemmatizer = WordNetLemmatizer()  # Initialize a lemmatizer for word stemming.\n",
        "\n",
        "stop_words = set(stop)  # Create a set of stopwords for filtering.\n",
        "word_tokens = word_tokenize(text.lower())  # Tokenize and convert text to lowercase.\n",
        "filtered_words = [w for w in word_tokens if w.isalnum()]  # Remove punctuation.\n",
        "filtered_words = [w for w in filtered_words if not w in stop_words]  # Remove stopwords.\n",
        "\n",
        "# Lemmatize words\n",
        "lemmatized_words = [lemmatizer.lemmatize(w) for w in filtered_words]  # Perform lemmatization.\n",
        "\n",
        "# Remove stop words after lemmatizer\n",
        "filtered_words2 = [w for w in lemmatized_words if not w in stop_words]  # Remove stopwords after lemmatization.\n",
        "\n",
        "# Count frequencies\n",
        "word_freq = Counter(filtered_words2)  # Count the frequency of each word.\n",
        "\n",
        "# Get most common words\n",
        "most_common_words = word_freq.most_common(num_words)  # Get the 30 most common words.\n",
        "\n",
        "df = pd.DataFrame(most_common_words, columns=['Word', 'Count'])  # Create a DataFrame to display the word frequencies.\n",
        "\n",
        "print(df)  # Display the DataFrame with word frequencies.\n",
        "\n",
        "\n",
        "# The purpose of this chunk is to generate an output filename based on the input filename and save the DataFrame as a CSV file with that name.\n",
        "from google.colab import files  # Import the \"files\" module from the \"google.colab\" library to work with files in Google Colab.\n",
        "import os  # Import the \"os\" module for operating system-related functions.\n",
        "\n",
        "name, ext = os.path.splitext(fullname)  # Split the \"fullname\" (presumably a file name) into its base name (\"name\") and extension (\"ext\").\n",
        "name2, ext2 = os.path.splitext(filename)  # Split the \"fullname\" (presumably a file name) into its base name (\"name\") and extension (\"ext\").\n",
        "# \"name\" now contains the base name of the file without its extension.\n",
        "\n",
        "# Create a variable with the file name and append \"_top_words.csv\" to it for the output file.\n",
        "output_filename = name + '_top_words.csv'\n",
        "print_name = name2 + '_top_words.csv'\n",
        "\n",
        "# Save the DataFrame \"df\" as a CSV file with the filename \"output_filename\" and exclude the index column.\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(\"Your output file has been written.  Its name is \" + print_name)\n"
      ],
      "metadata": {
        "id": "_qM4glLrXO5w",
        "outputId": "be9c03d6-1236-4c0e-ea14-a0f32b1689f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter file name : DGA_2010.txt\n",
            "File read successfully!\n",
            "Here are the first 100 characters of your file:\n",
            "This publication may be viewed and downloaded from the Internet at \n",
            "www.dietaryguidelines.gov. \n",
            "Sugg\n",
            "         Word  Count\n",
            "0           e   3833\n",
            "1           n   2974\n",
            "2           l   1640\n",
            "3           c   1533\n",
            "4           r   1520\n",
            "5           h   1407\n",
            "6           u   1196\n",
            "7           p    852\n",
            "8           f    832\n",
            "9           g    673\n",
            "10       food    556\n",
            "11          b    532\n",
            "12          2    352\n",
            "13          1    340\n",
            "14    dietary    247\n",
            "15    calorie    246\n",
            "16          w    239\n",
            "17          3    239\n",
            "18        fat    217\n",
            "19   american    210\n",
            "20         ra    210\n",
            "21         ro    199\n",
            "22          v    194\n",
            "23      grain    189\n",
            "24     intake    187\n",
            "25  guideline    165\n",
            "26       milk    156\n",
            "27          5    152\n",
            "28    pattern    139\n",
            "29          k    136\n",
            "Your output file has been written.  Its name is DGA_2010_top_words.csv\n"
          ]
        }
      ]
    }
  ]
}